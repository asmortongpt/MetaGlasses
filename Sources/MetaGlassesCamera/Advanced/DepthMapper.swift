import UIKit
import CoreImage
import Vision
import ARKit

/// Advanced 3D depth mapping from stereoscopic images
/// Creates actual 3D models from stereo pairs
public class DepthMapper {

    private let ciContext: CIContext

    public init() {
        self.ciContext = CIContext(options: [
            .workingColorSpace: CGColorSpace(name: CGColorSpace.sRGB)!,
            .useSoftwareRenderer: false
        ])
    }

    /// Generate depth map from stereo pair
    /// Returns a grayscale depth image (white=close, black=far)
    public func generateDepthMap(left: UIImage, right: UIImage) async throws -> DepthMapResult {
        guard let leftCG = left.cgImage, let rightCG = right.cgImage else {
            throw DepthError.invalidImages
        }

        print("ðŸŽ¯ Generating depth map from stereo pair...")

        // Step 1: Feature matching between left and right images
        let disparityMap = try await calculateDisparity(leftImage: leftCG, rightImage: rightCG)

        // Step 2: Convert disparity to depth
        let depthMap = disparityToDepth(disparity: disparityMap)

        // Step 3: Smooth and refine
        let refined = refineDepthMap(depthMap)

        // Step 4: Generate 3D point cloud
        let pointCloud = generatePointCloud(depthMap: refined, leftImage: leftCG)

        print("âœ… Depth map generated: \(pointCloud.count) 3D points")

        return DepthMapResult(
            depthImage: refined,
            pointCloud: pointCloud,
            minDepth: 0.5,
            maxDepth: 10.0
        )
    }

    /// Calculate disparity between stereo images using feature matching
    private func calculateDisparity(leftImage: CGImage, rightImage: CGImage) async throws -> CIImage {
        // Use Vision framework for feature detection
        let leftFeatures = try await detectFeatures(in: leftImage)
        let rightFeatures = try await detectFeatures(in: rightImage)

        // Match features between images
        let matches = matchFeatures(left: leftFeatures, right: rightFeatures)

        // Build disparity map from matches
        let disparityMap = buildDisparityMap(
            from: matches,
            leftSize: CGSize(width: leftImage.width, height: leftImage.height)
        )

        return disparityMap
    }

    private func detectFeatures(in image: CGImage) async throws -> [VNFeatureObservation] {
        return try await withCheckedThrowingContinuation { continuation in
            let request = VNDetectHorizonRequest()

            // Use contour detection for features
            let contourRequest = VNDetectContoursRequest()
            contourRequest.contrastAdjustment = 2.0
            contourRequest.detectsDarkOnLight = true

            let handler = VNImageRequestHandler(cgImage: image, options: [:])

            DispatchQueue.global(qos: .userInitiated).async {
                do {
                    try handler.perform([contourRequest])

                    if let results = contourRequest.results {
                        continuation.resume(returning: results)
                    } else {
                        continuation.resume(returning: [])
                    }
                } catch {
                    continuation.resume(throwing: error)
                }
            }
        }
    }

    private func matchFeatures(left: [VNFeatureObservation], right: [VNFeatureObservation]) -> [(CGPoint, CGFloat)] {
        // Simplified feature matching - in production, use SIFT/ORB
        var matches: [(CGPoint, CGFloat)] = []

        for leftFeature in left {
            let leftPoint = leftFeature.boundingBox.origin

            // Find closest feature in right image (epipolar constraint)
            for rightFeature in right {
                let rightPoint = rightFeature.boundingBox.origin

                // Features should be on same horizontal line (rectified stereo)
                if abs(leftPoint.y - rightPoint.y) < 0.05 {
                    let disparity = abs(leftPoint.x - rightPoint.x)
                    matches.append((leftPoint, disparity))
                }
            }
        }

        return matches
    }

    private func buildDisparityMap(from matches: [(CGPoint, CGFloat)], leftSize: CGSize) -> CIImage {
        // Create grayscale image from disparity values
        let width = Int(leftSize.width)
        let height = Int(leftSize.height)

        var pixels = [UInt8](repeating: 0, count: width * height)

        for (point, disparity) in matches {
            let x = Int(point.x * CGFloat(width))
            let y = Int(point.y * CGFloat(height))

            if x >= 0 && x < width && y >= 0 && y < height {
                let index = y * width + x
                // Map disparity (0-1) to grayscale (0-255)
                pixels[index] = UInt8(disparity * 255)
            }
        }

        // Create CIImage from pixel data
        let data = Data(pixels)
        let provider = CGDataProvider(data: data as CFData)!

        let cgImage = CGImage(
            width: width,
            height: height,
            bitsPerComponent: 8,
            bitsPerPixel: 8,
            bytesPerRow: width,
            space: CGColorSpaceCreateDeviceGray(),
            bitmapInfo: CGBitmapInfo(rawValue: CGImageAlphaInfo.none.rawValue),
            provider: provider,
            decode: nil,
            shouldInterpolate: false,
            intent: .defaultIntent
        )!

        return CIImage(cgImage: cgImage)
    }

    private func disparityToDepth(_ disparity: CIImage) -> UIImage {
        // Depth = baseline * focal_length / disparity
        // For Meta glasses, approximate baseline = 60mm

        guard let filter = CIFilter(name: "CIColorInvert") else {
            return UIImage(ciImage: disparity)
        }

        filter.setValue(disparity, forKey: kCIInputImageKey)

        let inverted = filter.outputImage ?? disparity

        guard let cgImage = ciContext.createCGImage(inverted, from: inverted.extent) else {
            return UIImage(ciImage: inverted)
        }

        return UIImage(cgImage: cgImage)
    }

    private func refineDepthMap(_ depthMap: UIImage) -> UIImage {
        guard let cgImage = depthMap.cgImage else { return depthMap }

        var ciImage = CIImage(cgImage: cgImage)

        // Apply bilateral filter for edge-preserving smoothing
        if let filter = CIFilter(name: "CIMorphologyGradient") {
            filter.setValue(ciImage, forKey: kCIInputImageKey)
            filter.setValue(3.0, forKey: kCIInputRadiusKey)
            ciImage = filter.outputImage ?? ciImage
        }

        guard let refined = ciContext.createCGImage(ciImage, from: ciImage.extent) else {
            return depthMap
        }

        return UIImage(cgImage: refined)
    }

    /// Generate 3D point cloud from depth map
    private func generatePointCloud(depthMap: UIImage, leftImage: CGImage) -> [Point3D] {
        guard let depthCG = depthMap.cgImage else { return [] }

        let width = depthCG.width
        let height = depthCG.height

        var points: [Point3D] = []

        // Sample every 10th pixel for performance
        let stride = 10

        for y in stride(from: 0, to: height, by: stride) {
            for x in stride(from: 0, to: width, by: stride) {
                if let depthValue = getPixelValue(from: depthCG, x: x, y: y),
                   let color = getPixelColor(from: leftImage, x: x, y: y) {

                    // Convert 2D + depth to 3D coordinates
                    let normalizedDepth = Float(depthValue) / 255.0
                    let depth = 0.5 + (normalizedDepth * 9.5) // Map to 0.5-10m range

                    // Perspective projection (simplified)
                    let focalLength: Float = 500.0
                    let cx = Float(width) / 2.0
                    let cy = Float(height) / 2.0

                    let xPos = ((Float(x) - cx) * depth) / focalLength
                    let yPos = ((Float(y) - cy) * depth) / focalLength
                    let zPos = depth

                    points.append(Point3D(
                        x: xPos,
                        y: yPos,
                        z: zPos,
                        color: color
                    ))
                }
            }
        }

        return points
    }

    private func getPixelValue(from image: CGImage, x: Int, y: Int) -> UInt8? {
        guard x >= 0 && x < image.width && y >= 0 && y < image.height else { return nil }

        guard let data = image.dataProvider?.data,
              let bytes = CFDataGetBytePtr(data) else { return nil }

        let bytesPerRow = image.bytesPerRow
        let index = y * bytesPerRow + x

        return bytes[index]
    }

    private func getPixelColor(from image: CGImage, x: Int, y: Int) -> UIColor? {
        guard x >= 0 && x < image.width && y >= 0 && y < image.height else { return nil }

        guard let data = image.dataProvider?.data,
              let bytes = CFDataGetBytePtr(data) else { return nil }

        let bytesPerPixel = image.bitsPerPixel / 8
        let bytesPerRow = image.bytesPerRow
        let index = y * bytesPerRow + x * bytesPerPixel

        let r = CGFloat(bytes[index]) / 255.0
        let g = CGFloat(bytes[index + 1]) / 255.0
        let b = CGFloat(bytes[index + 2]) / 255.0

        return UIColor(red: r, green: g, blue: b, alpha: 1.0)
    }
}

// MARK: - Supporting Types

public struct DepthMapResult {
    public let depthImage: UIImage
    public let pointCloud: [Point3D]
    public let minDepth: Float
    public let maxDepth: Float
}

public struct Point3D {
    public let x: Float
    public let y: Float
    public let z: Float
    public let color: UIColor
}

public enum DepthError: LocalizedError {
    case invalidImages
    case featureDetectionFailed
    case depthCalculationFailed

    public var errorDescription: String? {
        switch self {
        case .invalidImages:
            return "Invalid stereo images"
        case .featureDetectionFailed:
            return "Failed to detect features"
        case .depthCalculationFailed:
            return "Failed to calculate depth"
        }
    }
}
